{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Criando um Web App de Análise de Sentimento\r\n",
    "## Usando PyTorch e SageMaker\r\n",
    "\r\n",
    "_Esse tutorial foi feito com base no projeto final do Nanodegree em Deep Learning da Udacity_\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Agora que vocês já conhecem um pouco de AWS e de Sagemaker, iremos construir um projeto completo (end to end) com o intuito de desenvolver um web app onde o usuário insira um texto de avaliação de filme e saiba se essa avaliação é positiva ou negativa.\r\n",
    "\r\n",
    "## General Outline\r\n",
    "\r\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\r\n",
    "\r\n",
    "1. Download or otherwise retrieve the data.\r\n",
    "2. Process / Prepare the data.\r\n",
    "3. Upload the processed data to S3.\r\n",
    "4. Train a chosen model.\r\n",
    "5. Test the trained model (typically using a batch transform job).\r\n",
    "6. Deploy the trained model.\r\n",
    "7. Use the deployed model.\r\n",
    "\r\n",
    "For this project, you will be following the steps in the general outline with some modifications. \r\n",
    "\r\n",
    "First, you will not be testing the model in its own step. You will still be testing the model, however, you will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that you can make sure that your deployed model is working correctly before moving forward.\r\n",
    "\r\n",
    "In addition, you will deploy and use your trained model a second time. In the second iteration you will customize the way that your trained model is deployed by including some of your own code. In addition, your newly deployed model will be used in the sentiment analysis web app."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Make sure that we use SageMaker 1.x\r\n",
    "!pip install sagemaker==1.72.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.76)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.76 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.76)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.76->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.76->boto3>=1.14.12->sagemaker==1.72.0) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=e148b841145c44ca84a4b4c0559d270b36709a512f06072e33dd7eacb98666bb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.41.0\n",
      "    Uninstalling sagemaker-2.41.0:\n",
      "      Successfully uninstalled sagemaker-2.41.0\n",
      "Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 1: Baixando os dados\r\n",
    "\r\n",
    "Para o nosso projeto, usaremos uma base bastante conhecida chamada [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\r\n",
    "\r\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%mkdir ../data\r\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-05-31 23:46:19--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  8.31MB/s    in 13s     \n",
      "\n",
      "2021-05-31 23:46:33 (6.09 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 2: Preparando e processando os dados\r\n",
    "\r\n",
    "Processaremos os nossos dados a fim de deixá-los em um formato mais fácil para realizar o treinamento do modelo. Primeiramente, iremos unir as avaliação positivas e negativas em uma mesma estrutura de dados, uma vez que os dados vem em arquivos separados. Após isso iremos separá-los em treino e teste, garantindo que eles estejam misturados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "\r\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\r\n",
    "    data = {}\r\n",
    "    labels = {}\r\n",
    "    \r\n",
    "    for data_type in ['train', 'test']:\r\n",
    "        data[data_type] = {}\r\n",
    "        labels[data_type] = {}\r\n",
    "        \r\n",
    "        for sentiment in ['pos', 'neg']:\r\n",
    "            data[data_type][sentiment] = []\r\n",
    "            labels[data_type][sentiment] = []\r\n",
    "            \r\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\r\n",
    "            files = glob.glob(path)\r\n",
    "            \r\n",
    "            for f in files:\r\n",
    "                with open(f) as review:\r\n",
    "                    data[data_type][sentiment].append(review.read())\r\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\r\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\r\n",
    "                    \r\n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\r\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\r\n",
    "                \r\n",
    "    return data, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data, labels = read_imdb_data()\r\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\r\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\r\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.utils import shuffle\r\n",
    "\r\n",
    "def prepare_imdb_data(data, labels):\r\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\r\n",
    "    \r\n",
    "    #Combine positive and negative reviews and labels\r\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\r\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\r\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\r\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\r\n",
    "    \r\n",
    "    #Shuffle reviews and corresponding labels within training and test sets\r\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\r\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\r\n",
    "    \r\n",
    "    # Return a unified training data, test data, training labels, test labets\r\n",
    "    return data_train, data_test, labels_train, labels_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\r\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, vamos dar uma olhadinha nos nossos dados!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(train_X[100])\r\n",
    "print(train_y[100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Everyone else who has commented negatively about this film have done excellent analysis as to why this film is so bloody awful. I wasn't going to comment, but the film just bugs me so much, and the writer/director in particular. So I must toss in my hat to join the naysayers.<br /><br />I saw the original \"Wicker Man\" and really loved the cornucopia of music, sensuality, paganism in a modern world, and the clash of theological beliefs. This said, I am not part of the crowd that thinks remakes of great movies shouldn't be done. For example, I liked the original 1950's \"Invasion of the Body Snatchers\", but equally enjoyed the 1978 remake. Both films can stand on their own. Another example is \"The Thing\". The original, as campy as it looks compared to today's standards, has a lot to be proud of in the 1982 remake with Kurt Russell (my all time favorite horror movie). So that small minority of people who like \"The Wicker Man\" re-make can not accuse me of dissing this piece of crap just because it's a re-make.<br /><br />This film solidified for me Neil LaBute's sexism and misogynistic tendencies. It also made me wonder how executives, wanting to make a serious thriller, would green light a product that is so anti-female. There are too many scenes of Cage hitting women just because he's frustrated with them thwarting his investigation of a missing girl. would he react like this off the island in other cases where suspects aren't forthcoming? The original created a society in which men and women are equal participants in a Goddess based religion. The threat to the main character came from everyone, male and female. There was no sexual hierarchy.<br /><br />The metaphor of bees, drones etc was a bit heavy handed and convenient (\"The drone must die!\"), especially when Cage's character has bee allergies. I kept wondering why the men on the island didn't fight back and use mere physicality to stop these women from treating them like grunts. These were not women with special supernatural powers, and half of them seemed to be pregnant, the other half old and fat, and the rest girls and thin blonde waifs, so if the men really wanted to escape they could do what most men do when they hate women. Physically dominate them. There didn't seem to be any guns or weapons beyond cutting tools to hold them if they were unhappy. But if they were content being drones, why make them unable to speak? They could be used as a threat to Cage because they will defend the community. They are drones because Neil LaBute seems to believe that a society ran by women would leave men castrated. (That movie was made already. \"The Stepford Wives\" anyone?) Classic symptoms from men who are afraid of what may happen if women got their sh*t together and were truly equal citizens.<br /><br />The problem with the man-hating female society is that it makes uninteresting movie viewing and creates unintentional humor when Cage starts knocking women out. I belief LaBute should've left the society an egalitarian one, kept the sexuality and uninhibited lasciviousness, and pushed buttons of discomfort in regards to the children on that island. No one likes pedophiles or children to be sexually exploited. So how would a cop react if he saw lewd acts performed by adults with children around? There would be a logical mental leap that these children are abused, thus, an urgency created to save the missing child and get help for all the children. LaBute has said he created the fiancé and daughter story thread to give Cage's character an incentive to search. I don't think you need that. Any child abused will make an adult react to save them. The irony of course would be that the child Cage \"saves\" ultimately brings him death.<br /><br />The dialogue was contrived and campy. The whole third act was hilarious. The audience I saw it with guffawed (and later booed at the end). I just thought the movie started off wrong when the letter arrived written in the fancy handwriting and all the flashbacks cutting into to show how wounded Cage is. We don't need that. Just show him arriving on the island for an investigation of a missing child. Most of us in America have seen \"Law & Order\" and other cop procedurals. We come into the movie as if we are Cage's partner solving a mystery.<br /><br />So much potential...wasted. Neil LaBute, stick to talking head pictures for people who enjoy your male angst-ridden plays and flicks of that sort. Stay with your own company of men. Leave the thrillers for people who understand thrillers. Here is your jar of honey. I'll watch that.\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como primeiro passo de pré processamento, iremos limpar as tags HTML que podem aparecer nas avaliações e, após isso, iremos _tokenizar_ nossos dados para que palavras como *entertained* e *entertaining* sejam consideradas iguais no nosso modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "import nltk\r\n",
    "from nltk.stem.porter import *\r\n",
    "\r\n",
    "import re\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "def review_to_words(review):\r\n",
    "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\r\n",
    "    stemmer = PorterStemmer()\r\n",
    "    \r\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\r\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\r\n",
    "    words = text.split() # Split string into words\r\n",
    "    words = [w for w in words if w not in stopwords] # Remove stopwords\r\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\r\n",
    "    \r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A função `review_to_words` utiliza a biblioteca `BeautifulSoup` para remover as tags HTML e usa a biblioteca `nltk` para tokenizar as avaliações e remover as stopwords.  \r\n",
    "Abaixo, podemos ver o output dessa função aplicado a uma avaliação."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "review_to_words(train_X[100])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['everyon',\n",
       " 'els',\n",
       " 'comment',\n",
       " 'neg',\n",
       " 'film',\n",
       " 'done',\n",
       " 'excel',\n",
       " 'analysi',\n",
       " 'film',\n",
       " 'bloodi',\n",
       " 'aw',\n",
       " 'go',\n",
       " 'comment',\n",
       " 'film',\n",
       " 'bug',\n",
       " 'much',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'particular',\n",
       " 'must',\n",
       " 'toss',\n",
       " 'hat',\n",
       " 'join',\n",
       " 'naysay',\n",
       " 'saw',\n",
       " 'origin',\n",
       " 'wicker',\n",
       " 'man',\n",
       " 'realli',\n",
       " 'love',\n",
       " 'cornucopia',\n",
       " 'music',\n",
       " 'sensual',\n",
       " 'pagan',\n",
       " 'modern',\n",
       " 'world',\n",
       " 'clash',\n",
       " 'theolog',\n",
       " 'belief',\n",
       " 'said',\n",
       " 'part',\n",
       " 'crowd',\n",
       " 'think',\n",
       " 'remak',\n",
       " 'great',\n",
       " 'movi',\n",
       " 'done',\n",
       " 'exampl',\n",
       " 'like',\n",
       " 'origin',\n",
       " '1950',\n",
       " 'invas',\n",
       " 'bodi',\n",
       " 'snatcher',\n",
       " 'equal',\n",
       " 'enjoy',\n",
       " '1978',\n",
       " 'remak',\n",
       " 'film',\n",
       " 'stand',\n",
       " 'anoth',\n",
       " 'exampl',\n",
       " 'thing',\n",
       " 'origin',\n",
       " 'campi',\n",
       " 'look',\n",
       " 'compar',\n",
       " 'today',\n",
       " 'standard',\n",
       " 'lot',\n",
       " 'proud',\n",
       " '1982',\n",
       " 'remak',\n",
       " 'kurt',\n",
       " 'russel',\n",
       " 'time',\n",
       " 'favorit',\n",
       " 'horror',\n",
       " 'movi',\n",
       " 'small',\n",
       " 'minor',\n",
       " 'peopl',\n",
       " 'like',\n",
       " 'wicker',\n",
       " 'man',\n",
       " 'make',\n",
       " 'accus',\n",
       " 'diss',\n",
       " 'piec',\n",
       " 'crap',\n",
       " 'make',\n",
       " 'film',\n",
       " 'solidifi',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'sexism',\n",
       " 'misogynist',\n",
       " 'tendenc',\n",
       " 'also',\n",
       " 'made',\n",
       " 'wonder',\n",
       " 'execut',\n",
       " 'want',\n",
       " 'make',\n",
       " 'seriou',\n",
       " 'thriller',\n",
       " 'would',\n",
       " 'green',\n",
       " 'light',\n",
       " 'product',\n",
       " 'anti',\n",
       " 'femal',\n",
       " 'mani',\n",
       " 'scene',\n",
       " 'cage',\n",
       " 'hit',\n",
       " 'women',\n",
       " 'frustrat',\n",
       " 'thwart',\n",
       " 'investig',\n",
       " 'miss',\n",
       " 'girl',\n",
       " 'would',\n",
       " 'react',\n",
       " 'like',\n",
       " 'island',\n",
       " 'case',\n",
       " 'suspect',\n",
       " 'forthcom',\n",
       " 'origin',\n",
       " 'creat',\n",
       " 'societi',\n",
       " 'men',\n",
       " 'women',\n",
       " 'equal',\n",
       " 'particip',\n",
       " 'goddess',\n",
       " 'base',\n",
       " 'religion',\n",
       " 'threat',\n",
       " 'main',\n",
       " 'charact',\n",
       " 'came',\n",
       " 'everyon',\n",
       " 'male',\n",
       " 'femal',\n",
       " 'sexual',\n",
       " 'hierarchi',\n",
       " 'metaphor',\n",
       " 'bee',\n",
       " 'drone',\n",
       " 'etc',\n",
       " 'bit',\n",
       " 'heavi',\n",
       " 'hand',\n",
       " 'conveni',\n",
       " 'drone',\n",
       " 'must',\n",
       " 'die',\n",
       " 'especi',\n",
       " 'cage',\n",
       " 'charact',\n",
       " 'bee',\n",
       " 'allergi',\n",
       " 'kept',\n",
       " 'wonder',\n",
       " 'men',\n",
       " 'island',\n",
       " 'fight',\n",
       " 'back',\n",
       " 'use',\n",
       " 'mere',\n",
       " 'physic',\n",
       " 'stop',\n",
       " 'women',\n",
       " 'treat',\n",
       " 'like',\n",
       " 'grunt',\n",
       " 'women',\n",
       " 'special',\n",
       " 'supernatur',\n",
       " 'power',\n",
       " 'half',\n",
       " 'seem',\n",
       " 'pregnant',\n",
       " 'half',\n",
       " 'old',\n",
       " 'fat',\n",
       " 'rest',\n",
       " 'girl',\n",
       " 'thin',\n",
       " 'blond',\n",
       " 'waif',\n",
       " 'men',\n",
       " 'realli',\n",
       " 'want',\n",
       " 'escap',\n",
       " 'could',\n",
       " 'men',\n",
       " 'hate',\n",
       " 'women',\n",
       " 'physic',\n",
       " 'domin',\n",
       " 'seem',\n",
       " 'gun',\n",
       " 'weapon',\n",
       " 'beyond',\n",
       " 'cut',\n",
       " 'tool',\n",
       " 'hold',\n",
       " 'unhappi',\n",
       " 'content',\n",
       " 'drone',\n",
       " 'make',\n",
       " 'unabl',\n",
       " 'speak',\n",
       " 'could',\n",
       " 'use',\n",
       " 'threat',\n",
       " 'cage',\n",
       " 'defend',\n",
       " 'commun',\n",
       " 'drone',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'seem',\n",
       " 'believ',\n",
       " 'societi',\n",
       " 'ran',\n",
       " 'women',\n",
       " 'would',\n",
       " 'leav',\n",
       " 'men',\n",
       " 'castrat',\n",
       " 'movi',\n",
       " 'made',\n",
       " 'alreadi',\n",
       " 'stepford',\n",
       " 'wive',\n",
       " 'anyon',\n",
       " 'classic',\n",
       " 'symptom',\n",
       " 'men',\n",
       " 'afraid',\n",
       " 'may',\n",
       " 'happen',\n",
       " 'women',\n",
       " 'got',\n",
       " 'sh',\n",
       " 'togeth',\n",
       " 'truli',\n",
       " 'equal',\n",
       " 'citizen',\n",
       " 'problem',\n",
       " 'man',\n",
       " 'hate',\n",
       " 'femal',\n",
       " 'societi',\n",
       " 'make',\n",
       " 'uninterest',\n",
       " 'movi',\n",
       " 'view',\n",
       " 'creat',\n",
       " 'unintent',\n",
       " 'humor',\n",
       " 'cage',\n",
       " 'start',\n",
       " 'knock',\n",
       " 'women',\n",
       " 'belief',\n",
       " 'labut',\n",
       " 'left',\n",
       " 'societi',\n",
       " 'egalitarian',\n",
       " 'one',\n",
       " 'kept',\n",
       " 'sexual',\n",
       " 'uninhibit',\n",
       " 'lascivi',\n",
       " 'push',\n",
       " 'button',\n",
       " 'discomfort',\n",
       " 'regard',\n",
       " 'children',\n",
       " 'island',\n",
       " 'one',\n",
       " 'like',\n",
       " 'pedophil',\n",
       " 'children',\n",
       " 'sexual',\n",
       " 'exploit',\n",
       " 'would',\n",
       " 'cop',\n",
       " 'react',\n",
       " 'saw',\n",
       " 'lewd',\n",
       " 'act',\n",
       " 'perform',\n",
       " 'adult',\n",
       " 'children',\n",
       " 'around',\n",
       " 'would',\n",
       " 'logic',\n",
       " 'mental',\n",
       " 'leap',\n",
       " 'children',\n",
       " 'abus',\n",
       " 'thu',\n",
       " 'urgenc',\n",
       " 'creat',\n",
       " 'save',\n",
       " 'miss',\n",
       " 'child',\n",
       " 'get',\n",
       " 'help',\n",
       " 'children',\n",
       " 'labut',\n",
       " 'said',\n",
       " 'creat',\n",
       " 'fianc',\n",
       " 'daughter',\n",
       " 'stori',\n",
       " 'thread',\n",
       " 'give',\n",
       " 'cage',\n",
       " 'charact',\n",
       " 'incent',\n",
       " 'search',\n",
       " 'think',\n",
       " 'need',\n",
       " 'child',\n",
       " 'abus',\n",
       " 'make',\n",
       " 'adult',\n",
       " 'react',\n",
       " 'save',\n",
       " 'ironi',\n",
       " 'cours',\n",
       " 'would',\n",
       " 'child',\n",
       " 'cage',\n",
       " 'save',\n",
       " 'ultim',\n",
       " 'bring',\n",
       " 'death',\n",
       " 'dialogu',\n",
       " 'contriv',\n",
       " 'campi',\n",
       " 'whole',\n",
       " 'third',\n",
       " 'act',\n",
       " 'hilari',\n",
       " 'audienc',\n",
       " 'saw',\n",
       " 'guffaw',\n",
       " 'later',\n",
       " 'boo',\n",
       " 'end',\n",
       " 'thought',\n",
       " 'movi',\n",
       " 'start',\n",
       " 'wrong',\n",
       " 'letter',\n",
       " 'arriv',\n",
       " 'written',\n",
       " 'fanci',\n",
       " 'handwrit',\n",
       " 'flashback',\n",
       " 'cut',\n",
       " 'show',\n",
       " 'wound',\n",
       " 'cage',\n",
       " 'need',\n",
       " 'show',\n",
       " 'arriv',\n",
       " 'island',\n",
       " 'investig',\n",
       " 'miss',\n",
       " 'child',\n",
       " 'us',\n",
       " 'america',\n",
       " 'seen',\n",
       " 'law',\n",
       " 'order',\n",
       " 'cop',\n",
       " 'procedur',\n",
       " 'come',\n",
       " 'movi',\n",
       " 'cage',\n",
       " 'partner',\n",
       " 'solv',\n",
       " 'mysteri',\n",
       " 'much',\n",
       " 'potenti',\n",
       " 'wast',\n",
       " 'neil',\n",
       " 'labut',\n",
       " 'stick',\n",
       " 'talk',\n",
       " 'head',\n",
       " 'pictur',\n",
       " 'peopl',\n",
       " 'enjoy',\n",
       " 'male',\n",
       " 'angst',\n",
       " 'ridden',\n",
       " 'play',\n",
       " 'flick',\n",
       " 'sort',\n",
       " 'stay',\n",
       " 'compani',\n",
       " 'men',\n",
       " 'leav',\n",
       " 'thriller',\n",
       " 'peopl',\n",
       " 'understand',\n",
       " 'thriller',\n",
       " 'jar',\n",
       " 'honey',\n",
       " 'watch']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Já a função abaixo, `preprocess_data`, aplica a função `review_to_words` para cada uma das avaliações dos datasets de treino e teste. Além disso, ela também faz o cache dos dados, para que, caso algo aconteça, você possa voltar o pré processamento de onde parou."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\r\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\r\n",
    "\r\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\r\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\r\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\r\n",
    "\r\n",
    "    # If cache_file is not None, try to read from it first\r\n",
    "    cache_data = None\r\n",
    "    if cache_file is not None:\r\n",
    "        try:\r\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\r\n",
    "                cache_data = pickle.load(f)\r\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\r\n",
    "        except:\r\n",
    "            pass  # unable to read from cache, but that's okay\r\n",
    "    \r\n",
    "    # If cache is missing, then do the heavy lifting\r\n",
    "    if cache_data is None:\r\n",
    "        # Preprocess training and test data to obtain words for each review\r\n",
    "        #words_train = list(map(review_to_words, data_train))\r\n",
    "        #words_test = list(map(review_to_words, data_test))\r\n",
    "        words_train = [review_to_words(review) for review in data_train]\r\n",
    "        words_test = [review_to_words(review) for review in data_test]\r\n",
    "        \r\n",
    "        # Write to cache file for future runs\r\n",
    "        if cache_file is not None:\r\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\r\n",
    "                              labels_train=labels_train, labels_test=labels_test)\r\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\r\n",
    "                pickle.dump(cache_data, f)\r\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\r\n",
    "    else:\r\n",
    "        # Unpack data loaded from cache file\r\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\r\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\r\n",
    "    \r\n",
    "    return words_train, words_test, labels_train, labels_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Preprocess data\r\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformando os dados\r\n",
    "\r\n",
    "Agora, nós iremos construir uma representação dos nossos dados muito similar a representação conhecida como bag-of-words. Para a rede neural recorrente que iremos usar, nós iremos relizar a transformação dos dados da seguinte forma:\r\n",
    "\r\n",
    "1. Transformar cada palavra em um número inteiro;\r\n",
    "2. Definir um tamanho para nosso vocabulário, ou seja, iremos remover palavras que aparecem pouco (para essas palavras atribuiremos o mesmo número inteiro (1));\r\n",
    "3. Como estamos usando uma RNN, defineros um tamanho para nossas sequência, ou seja, truncaremos aquelas que forem maiores e iremos inserir um caractér (0) para quando a avaliação for menor do que o tamanho definido."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Começaremos construindo uma função que nos retorna um dicionário de tamanho especifíco e com as palavras que mais aparecem. Não podemos esquecer de reservar o índice 0 e 1 para os caractéres vazio e pouco frequente!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def build_dict(data, vocab_size = 5000):\r\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\r\n",
    "    \r\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\r\n",
    "    for review in data:\r\n",
    "        for word in review:\r\n",
    "            if word_count.get(word) is None:\r\n",
    "                word_count[word] = 1\r\n",
    "            else:\r\n",
    "                word_count[word] += 1\r\n",
    "    \r\n",
    "    sorted_words = [k for k,v in sorted(word_count.items(), key=lambda item: item[1], reverse=True)]\r\n",
    "    \r\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\r\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\r\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\r\n",
    "        \r\n",
    "    return word_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "word_dict = build_dict(train_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Salvando nosso dicionário\r\n",
    "\r\n",
    "Mais para frente, quando tivermos nosso modelo, teremos que usar nosso dicionário para realizar as predições. Sendo assim, precisamos salvá-lo para usar no futuro!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\r\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\r\n",
    "    os.makedirs(data_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\r\n",
    "    pickle.dump(word_dict, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformando as avaliações\r\n",
    "\r\n",
    "Agora, é hora de convertermos nossas avaliações de treino e teste para a sequência de número inteiros de tamanho fixo que entrará na nossa rede neural!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\r\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\r\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\r\n",
    "    \r\n",
    "    working_sentence = [NOWORD] * pad\r\n",
    "    \r\n",
    "    for word_index, word in enumerate(sentence[:pad]):\r\n",
    "        if word in word_dict:\r\n",
    "            working_sentence[word_index] = word_dict[word]\r\n",
    "        else:\r\n",
    "            working_sentence[word_index] = INFREQ\r\n",
    "            \r\n",
    "    return working_sentence, min(len(sentence), pad)\r\n",
    "\r\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\r\n",
    "    result = []\r\n",
    "    lengths = []\r\n",
    "    \r\n",
    "    for sentence in data:\r\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\r\n",
    "        result.append(converted)\r\n",
    "        lengths.append(leng)\r\n",
    "        \r\n",
    "    return np.array(result), np.array(lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\r\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apenas para validar se nossa função está fazendo tudo corretamente, vamos verificar um dado da nossa base de treino e ver seu tamanho."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\r\n",
    "print(train_X[100])\r\n",
    "print(len(train_X[100]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 226  265  329 1287    3  143  222 3939    3 1559  299   25  329    3\n",
      " 1694   21  308   67  765  130 2932 1735 1352    1  135   90    1   55\n",
      "   16   29    1   85 4638    1  583   92 4147    1 1713  233   63 1611\n",
      "   30  822   26    2  143  360    5   90 1483 3514  458    1  866   77\n",
      " 4322  822    3  390   79  360   35   90 2253   19  509  433  695   70\n",
      " 2320    1  822 2734 1955    6  395  102    2  340 1079   23    5    1\n",
      "   55    8 2166    1  267  522    8    3    1 2928    1    1    1 4291\n",
      "   27   34  108  826   50    8  566  529   15 1134  359  217 1104  544\n",
      "   46   18 1491  382  305 1479    1 1170  240   89   15 2547    5  915\n",
      "  326 1030    1   90  331  795  283  305  866 2221    1  341 1808 2752\n",
      "  216    9  333  226  774  544  556    1 2843    1 4974  453  124 1052\n",
      "  228 2981 4974  130  262  187 1491    9    1    1  749  108  283  915\n",
      "  257   64   65 1001 1014  368  305  775    5    1  305  221 2135  271\n",
      "  252   39 2418  252   72 1701  302   89 1383 1318    1  283   16   50\n",
      "  640   36  283  427  305 1014 1734   39  648 1549  652  374 2776  521\n",
      " 3539 1260 4974    8 1889  482   36   65 2752 1491 2248  980 4974 2928\n",
      "    1   39   94  795 1938  305   15  219  283    1    2   34  404    1\n",
      " 3466  181  242    1  283 1463  121  109  305  111 3541  223  317  866\n",
      " 2204  204   55  427  544  795    8 2111    2  234  331 1913  351 1491\n",
      "   87 1578  305 1713    1  245  795    1    4  749  556    1    1 1235\n",
      " 2891    1 1111  375  915    4    5    1  375  556 1070   15  602 2547\n",
      "  135    1   32   60  642  375  110   15 1304  997 3300  375 1297 1246\n",
      "    1  331  319  240  422   10  163  375    1  233  331 2799  423   13\n",
      " 3170   57 1491    9    1 1032   30  122  422 1297    8  642 2547  319\n",
      " 2554  190   15  422 1491  319  732  324  248  338 1774 2253  144  721\n",
      "   32  499  179  135    1  231 4826   22   99    2   87  294 1965 1038\n",
      "  343 2772    1 1250  374   20 1915 1491  122   20 1038  915 1170  240\n",
      "  422   98  762   47  903  471  602    1   45    2 1491 1432 1972  397\n",
      "   21  790  224 2928    1  860  241  253  269   23   77  774 4518 3940\n",
      "   33  342  330  441  918  283  219  529   23  244  529 3092    1   12\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "500\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 3: Subindo nossos dados de treino para o S3\r\n",
    "\r\n",
    "Nós iremos precisar subir nossos dados de treino para o S3 para que possamos acessá-lo durante o treinamento.\r\n",
    "\r\n",
    "### Salvando os dados de treino localmente\r\n",
    "\r\n",
    "Antes de subir para o S3, iremos salvar nossos dados localmente. É muito importante saber a estrutura dos dados que vamos salvar, para que possamos utilizar de forma correta. No nosso caso, as linhas do nosso dataset irão ter a forma (colunas): `label`, `length`, `review[500]`, onde `review[500]`é a sequência com 500 números inteiros que geramos acima."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import pandas as pd\r\n",
    "    \r\n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\r\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subindo para o S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import sagemaker\r\n",
    "\r\n",
    "sagemaker_session = sagemaker.Session()\r\n",
    "\r\n",
    "bucket = sagemaker_session.default_bucket()\r\n",
    "prefix = 'sagemaker/sentiment_rnn'\r\n",
    "\r\n",
    "role = sagemaker.get_execution_role()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OBS:** A célula acima sobe todos os arquivos contidos no nosso diretório para o S3. Isso inclui o `word_dict.pkl`, que usaremos na hora de realizar uma nova predição e garantir que o pré processamento dos novos dados seja o mesmo dos dados de treino. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 4: Construindo e treinando nosso modelo em Pytorch\r\n",
    "\r\n",
    "In the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects\r\n",
    "\r\n",
    " - Model Artifacts,\r\n",
    " - Training Code, and\r\n",
    " - Inference Code,\r\n",
    " \r\n",
    "each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.\r\n",
    "\r\n",
    "We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the `model.py` file, inside of the `train` folder. You can see the provided implementation by running the cell below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "!pygmentize train/model.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The important takeaway from the implementation provided is that there are three parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. We will likely want to make these parameters configurable in the training script so that if we wish to modify them we do not need to modify the script itself. We will see how to do this later on. To start we will write some of the training code in the notebook so that we can more easily diagnose any issues that arise.\n",
    "\n",
    "First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import torch\r\n",
    "import torch.utils.data\r\n",
    "\r\n",
    "# Read in only the first 250 rows\r\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\r\n",
    "\r\n",
    "# Turn the input pandas dataframe into tensors\r\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\r\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\r\n",
    "\r\n",
    "# Build the dataset\r\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\r\n",
    "# Build the dataloader\r\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (TODO) Writing the training method\n",
    "\n",
    "Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\r\n",
    "    for epoch in range(1, epochs + 1):\r\n",
    "        model.train()\r\n",
    "        total_loss = 0\r\n",
    "        for batch in train_loader:         \r\n",
    "            batch_X, batch_y = batch\r\n",
    "            \r\n",
    "            batch_X = batch_X.to(device)\r\n",
    "            batch_y = batch_y.to(device)\r\n",
    "            \r\n",
    "            optimizer.zero_grad()\r\n",
    "            output = model.forward(batch_X)\r\n",
    "            loss = loss_fn(output, batch_y)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            \r\n",
    "            total_loss += loss.data.item()\r\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import torch.optim as optim\r\n",
    "from train.model import LSTMClassifier\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\r\n",
    "optimizer = optim.Adam(model.parameters())\r\n",
    "loss_fn = torch.nn.BCELoss()\r\n",
    "\r\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, BCELoss: 0.6934759020805359\n",
      "Epoch: 2, BCELoss: 0.6848492383956909\n",
      "Epoch: 3, BCELoss: 0.6776467561721802\n",
      "Epoch: 4, BCELoss: 0.6698715329170227\n",
      "Epoch: 5, BCELoss: 0.6606590151786804\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (TODO) Training the model\n",
    "\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the `train()` method which you wrote earlier in this notebook.\n",
    "\n",
    "**TODO**: Copy the `train()` method written above and paste it into the `train/train.py` file where required.\n",
    "\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from sagemaker.pytorch import PyTorch\r\n",
    "\r\n",
    "estimator = PyTorch(entry_point=\"train.py\",\r\n",
    "                    source_dir=\"train\",\r\n",
    "                    role=role,\r\n",
    "                    framework_version='0.4.0',\r\n",
    "                    train_instance_count=1,\r\n",
    "                    train_instance_type='ml.c4.xlarge',\r\n",
    "                    hyperparameters={\r\n",
    "                        'epochs': 10,\r\n",
    "                        'hidden_dim': 200,\r\n",
    "                    })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "estimator.fit({'training': input_data})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-01 00:57:13 Starting - Starting the training job...\n",
      "2021-06-01 00:57:14 Starting - Launching requested ML instances......\n",
      "2021-06-01 00:58:24 Starting - Preparing the instances for training......\n",
      "2021-06-01 00:59:29 Downloading - Downloading input data...\n",
      "2021-06-01 00:59:54 Training - Downloading the training image.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,473 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,476 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,487 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,491 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:16,747 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/38/3f/4c42a98c9ad7d08c16e7d23b2194a0e4f3b2914662da8bc88986e4e6de1f/regex-2021.4.4.tar.gz (693kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/42/d7/f357d98e9b50346bcb6095fe3ad205d8db3174eb5edb03edfe7c4099576d/tqdm-4.61.0-py2.py3-none-any.whl (75kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvgacggc/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/05/a8/b85fa0bd7850b99f9b4f106972975f2e3c46412e12f9949b58\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, regex, joblib, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\n",
      "2021-06-01 01:00:15 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed beautifulsoup4-4.9.3 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.1 regex-2021.4.4 soupsieve-2.1 tqdm-4.61.0 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:36,022 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-01 01:00:36,035 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"log_level\": 20,\n",
      "    \"module_dir\": \"s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-06-01-00-57-12-739\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"num_cpus\": 4,\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-06-01-00-57-12-739\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-sa-east-1-275448561531/sagemaker-pytorch-2021-06-01-00-57-12-739/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 0.6758968538167526\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.589828126284541\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.5041245781645483\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.4260524918838423\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.3772974458276009\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.36206504824210184\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.3235926025984239\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.3144906558552567\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.2952807351034515\u001b[0m\n",
      "\n",
      "2021-06-01 02:34:07 Uploading - Uploading generated training model\n",
      "2021-06-01 02:34:07 Completed - Training job completed\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.2743338230921298\u001b[0m\n",
      "\u001b[34m2021-06-01 02:33:59,485 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 5678\n",
      "Billable seconds: 5678\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Testing the model\n",
    "\n",
    "As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.\n",
    "\n",
    "## Step 6: Deploy the model for testing\n",
    "\n",
    "Now that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form `review_length, review[500]` where `review[500]` is a sequence of `500` integers which describe the words present in the review, encoded using `word_dict`. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this.\n",
    "\n",
    "There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made.\n",
    "\n",
    "**NOTE**: When the built-in inference code is run it must import the `model_fn()` method from the `train.py` file. This is why the training code is wrapped in a main guard ( ie, `if __name__ == '__main__':` )\n",
    "\n",
    "Since we don't need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is.\n",
    "\n",
    "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**\n",
    "\n",
    "**TODO:** Deploy the trained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# TODO: Deploy the trained model\r\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7 - Use the model for testing\n",
    "\n",
    "Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\r\n",
    "\r\n",
    "def predict(data, rows=512):\r\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\r\n",
    "    predictions = np.array([])\r\n",
    "    for array in split_array:\r\n",
    "        predictions = np.append(predictions, predictor.predict(array))\r\n",
    "    \r\n",
    "    return predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "predictions = predict(test_X.values)\r\n",
    "predictions = [round(num) for num in predictions]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "accuracy_score(test_y, predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.85348"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question:** How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do *you* think is better for sentiment analysis?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer:**\n",
    "\n",
    "Acc XGBoost: 0.86116\n",
    "\n",
    "Acc RNN: 0.85348\n",
    "\n",
    "As we can see, the two models achieve very close accuracy, but we train only for 10 epochs our RNN and, maybe, if we train for more epochs we can achieve higher accuracy than in the XGBoost. I think that RNN will perform better due to the nature of the problem, which is a liguistic problem, so, the order of the words in a sentece have a huge impact in the final prediction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (TODO) More testing\n",
    "\n",
    "We now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The question we now need to answer is, how do we send this review to our model?\n",
    "\n",
    "Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews.\n",
    " - Removed any html tags and stemmed the input\n",
    " - Encoded the review as a sequence of integers using `word_dict`\n",
    " \n",
    "In order process the review we will need to repeat these two steps.\n",
    "\n",
    "**TODO**: Using the `review_to_words` and `convert_and_pad` methods from section one, convert `test_review` into a numpy array `test_data` suitable to send to our model. Remember that our model expects input of the form `review_length, review[500]`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# TODO: Convert test_review into a form usable by the model and save the results in test_data\r\n",
    "test_data, len_test_data = convert_and_pad(word_dict, review_to_words(test_review))\r\n",
    "test_data = np.array(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "predictor.predict([test_data])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(0.6170426, dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the return value of our model is close to `1`, we can be certain that the review we submitted is positive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Of course, just like in the XGBoost notebook, once we've deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "estimator.delete_endpoint()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6 (again) - Deploy the model for the web app\n",
    "\n",
    "Now that we know that our model is working, it's time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.\n",
    "\n",
    "As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code.\n",
    "\n",
    "We will store the code that we write in the `serve` directory. Provided in this directory is the `model.py` file that we used to construct our model, a `utils.py` file which contains the `review_to_words` and `convert_and_pad` pre-processing functions which we used during the initial data processing, and `predict.py`, the file which will contain our custom inference code. Note also that `requirements.txt` is present which will tell SageMaker what Python libraries are required by our custom inference code.\n",
    "\n",
    "When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use.\n",
    " - `model_fn`: This function is the same function that we used in the training script and it tells SageMaker how to load our model.\n",
    " - `input_fn`: This function receives the raw serialized input that has been sent to the model's endpoint and its job is to de-serialize and make the input available for the inference code.\n",
    " - `output_fn`: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model's endpoint.\n",
    " - `predict_fn`: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.\n",
    "\n",
    "For the simple website that we are constructing during this project, the `input_fn` and `output_fn` methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.\n",
    "\n",
    "### (TODO) Writing inference code\n",
    "\n",
    "Before writing our custom inference code, we will begin by taking a look at the code which has been provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "!pygmentize serve/predict.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.word_dict = pickle.load(f)\n",
      "\n",
      "    model.to(device).eval()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\n",
      "\n",
      "    data_X = \u001b[34mNone\u001b[39;49;00m\n",
      "    data_len = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\n",
      "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\n",
      "    data_pack = np.hstack((data_len, data_X))\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "    data = torch.from_numpy(data_pack)\n",
      "    data = data.to(device)\n",
      "\n",
      "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\n",
      "    model.eval()\n",
      "\n",
      "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\n",
      "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\n",
      "\n",
      "    result = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned earlier, the `model_fn` method is the same as the one provided in the training code and the `input_fn` and `output_fn` methods are very simple and your task will be to complete the `predict_fn` method. Make sure that you save the completed file as `predict.py` in the `serve` directory.\n",
    "\n",
    "**TODO**: Complete the `predict_fn()` method in the `serve/predict.py` file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deploying the model\n",
    "\n",
    "Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.\n",
    "\n",
    "**NOTE**: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a `numpy` array. In our case we want to send a string so we need to construct a simple wrapper around the `RealTimePredictor` class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from sagemaker.predictor import RealTimePredictor\r\n",
    "from sagemaker.pytorch import PyTorchModel\r\n",
    "\r\n",
    "class StringPredictor(RealTimePredictor):\r\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\r\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\r\n",
    "\r\n",
    "model = PyTorchModel(model_data=estimator.model_data,\r\n",
    "                     role = role,\r\n",
    "                     framework_version='0.4.0',\r\n",
    "                     entry_point='predict.py',\r\n",
    "                     source_dir='serve',\r\n",
    "                     predictor_cls=StringPredictor)\r\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the model\n",
    "\n",
    "Now that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first `250` positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "import glob\r\n",
    "\r\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\r\n",
    "    \r\n",
    "    results = []\r\n",
    "    ground = []\r\n",
    "    \r\n",
    "    # We make sure to test both positive and negative reviews    \r\n",
    "    for sentiment in ['pos', 'neg']:\r\n",
    "        \r\n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\r\n",
    "        files = glob.glob(path)\r\n",
    "        \r\n",
    "        files_read = 0\r\n",
    "        \r\n",
    "        print('Starting ', sentiment, ' files')\r\n",
    "        \r\n",
    "        # Iterate through the files and send them to the predictor\r\n",
    "        for f in files:\r\n",
    "            with open(f) as review:\r\n",
    "                # First, we store the ground truth (was the review positive or negative)\r\n",
    "                if sentiment == 'pos':\r\n",
    "                    ground.append(1)\r\n",
    "                else:\r\n",
    "                    ground.append(0)\r\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\r\n",
    "                review_input = review.read().encode('utf-8')\r\n",
    "                # Send the review to the predictor and store the results\r\n",
    "                results.append(float(predictor.predict(review_input)))\r\n",
    "                \r\n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\r\n",
    "            # only send a small number of reviews\r\n",
    "            files_read += 1\r\n",
    "            if files_read == stop:\r\n",
    "                break\r\n",
    "            \r\n",
    "    return ground, results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "ground, results = test_reviews()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "accuracy_score(ground, results)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an additional test, we can try sending the `test_review` that we looked at earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "predictor.predict(test_review)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'1.0'"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don't have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Criando nossa função Lambda\r\n",
    "\r\n",
    "Construimos nosso endpoint e deployamos nosso modelo, mas... e se quisessemos que nosso modelo fosse acessável por meio de um webapp? Para isso, precisamos construir alguns outros componentes, que podemos ver na arquitetura abaixo:\r\n",
    "\r\n",
    "<img src=\"Web App Diagram.svg\">\r\n",
    "\r\n",
    "Indo da esquerda para direita temos: \r\n",
    "\r\n",
    "- A EC2 será responsável por subir nossa aplicação Flask, servir nossa página e enviar a requisição para o API Gateway;\r\n",
    "- O API Gateway receberá a requisição e encaminhará para a função Lambda;\r\n",
    "- Já a função Lambda funcionará como redirecionando o texto da nova avaliação para o Sagemaker endpoint, uma vez que não há como realizar a integração API Gateway -> Sagemaker Endpoint de forma nativa e também não podemos chamar a URL do Sagemaker Endpoint sem estar dentro da AWS.\r\n",
    "\r\n",
    "### Configurando nossa função Lambda\r\n",
    "\r\n",
    "A primeira coisa que faremos será a criação da nossa função Lambda. Nossa função receberá como input os dados vindos do nosso API Gateway, irá realizar a chamada do endpoint e retornará a resposta para o API Gateway.\r\n",
    "\r\n",
    "#### Parte A: Criando IAM Role para a Lambda\r\n",
    "\r\n",
    "Já que queremos que nossa função Lambda chame o Sagemaker Endpoint, precisamos garantir que ele tenha permissão para isso. Dessa forma, precisamos adicionar essa permissão dentro da Role que usaremos em nossa função Lambda.\r\n",
    "\r\n",
    "Usando o Console da AWS, procuraremos por **IAM** na barra de busca e clicaremos em **Roles** no menu esquerdo. Feito isso, clique em **Create Role**. Garante que em **AWS service** o time de _trusted entity_ selecionado seja **Lambda** e, em seguida, clique em **Next: Permissions**.\r\n",
    "\r\n",
    "Na barra de busca procure por `sagemaker` e clique no checkbox referente a **AmazonSageMakerFullAccess** policy. Clique em **Next: Review**.\r\n",
    "\r\n",
    "Por último, dê um nome para usa role e garanta que você irá se lembrar na hora de criar sua função Lambda!\r\n",
    "\r\n",
    "Usaremos o nome `LambdaSageMakerRole`.\r\n",
    "\r\n",
    "#### Parte B: Criando a Lambda\r\n",
    "\r\n",
    "Agora, é hora de criarmos nossa função Lambda!\r\n",
    "\r\n",
    "Para isso, no console AWS, procure por Lambda e clique em **Create a function**. Na página seguinte, clique em **Author from scratch**, selecione o runtime como sendo Python, dê um nome para sua função, como por exemplo: `sentiment_analysis_func`. Não esqueça de selecionar a role que criamos!\r\n",
    "\r\n",
    "Após isso, clique em **Create Function**.\r\n",
    "\r\n",
    "Na próxima página você verá algumas informações sobre sua função Lambda que você acabou de criar. Se você descer um pouco a página, verá um editor de texto onde você pode escrever código que será executado quando sua função for chamada. No nosso projeto, usaremos o código abaixo (basta copiar e colar) e não esqueça de mudar o `EndpointName` para o nome do endpoint que pegaremos na célula abaixo.\r\n",
    "\r\n",
    "```python\r\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\r\n",
    "# is not available natively through Lambda.\r\n",
    "import boto3\r\n",
    "\r\n",
    "def lambda_handler(event, context):\r\n",
    "\r\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\r\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\r\n",
    "\r\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\r\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\r\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\r\n",
    "                                       Body = event['body'])                       # The actual review\r\n",
    "\r\n",
    "    # The response is an HTTP response whose body contains the result of our inference\r\n",
    "    result = response['Body'].read().decode('utf-8')\r\n",
    "\r\n",
    "    return {\r\n",
    "        'statusCode' : 200,\r\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\r\n",
    "        'body' : result\r\n",
    "    }\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "predictor.endpoint"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2021-06-01-03-42-13-849'"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Após alterar o nome do endpoint na sua função Lambda, clique em **Salvar**. Feito, sua função Lambda estará pronto para ser executada!\r\n",
    "\r\n",
    "### Criando e configurando o API Gateway\r\n",
    "\r\n",
    "Agora, é hora de criarmos nosso API Gateway!\r\n",
    "\r\n",
    "Abra o console da AWS e busque por **API Gateway** e, na página inicial, procure por REST API e clique em **Build**. Na tela de configuração, em **Create new API** selecione **New API** e defina um nome para seu Gateway e clique em **Create API**.\r\n",
    "\r\n",
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\r\n",
    "\r\n",
    "Using AWS Console, navigate to **Amazon API Gateway** and then click on **Get started**. \r\n",
    "\r\n",
    "On the next page, make sure that **New API** is selected and give the new api a name, for example, `sentiment_analysis_api`. Then, click on **Create API**.\r\n",
    "\r\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\r\n",
    "\r\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, select its dropdown menu and select **POST**, then click on the check mark beside it.\r\n",
    "\r\n",
    "For the integration point, make sure that **Lambda Function** is selected and click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\r\n",
    "\r\n",
    "Type the name of the Lambda function you created earlier into the **Lambda Function** text entry box and then click on **Save**. Click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\r\n",
    "\r\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and click on **Deploy API**. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\r\n",
    "\r\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Deployando nosso web app\r\n",
    "\r\n",
    "Agora, como passo final, faremos o deploy do nosso web app, para que possamos acessá-lo pelo browser e saber o sentimento de novas avaliações de filmes!  \r\n",
    "Para isso, usaremos uma instância de EC2 e subiremos uma aplicação Flask que será responsável por carregar a página e fazer as chamadas no API Gateway.\r\n",
    "\r\n",
    "Assim, primeiramente, é necessário criar criar uma instância de EC2 e configurar seu `security group` para garantir que seja permitido o ingresso pela porta 80 (Adicionar uma inbound rule permitindo HTTP para qualquer Ipv4).\r\n",
    "\r\n",
    "Após isso, vamos acessar, via browser, o terminal da máquina que criamos e instalar o git (```yum install git```). Com o git instalado, iremos executar o comando ```git clone https://github.com/pedrogengo/aws-deploy-example.git```.\r\n",
    "\r\n",
    "Feito isso, iremos acessar a pasta `website\\templates` e abrir o arquivo chamado `index.html` (```vim index.html```) para que possamos alterar a URL da nossa API. Procure por uma linha que contém **\\*\\*REPLACE WITH PUBLIC API URL\\*\\*** e insira a url da sua API onde será possível realizar a predição. Feito isso, salve o arquivo (```Esc -> :wq```) e volte para o diretório `website` (```cd ..```)`.\r\n",
    "\r\n",
    "Já no repositório `website`, execute o comando ```pip3 install -r requirements.txt``` e, depois disso, o comando ```python3 -m flask run --host 0.0.0.0 --port 80```.\r\n",
    "\r\n",
    "Pronto! Basta acessar o Ip da sua máquina e ver sua aplicação funcionando!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "predictor.delete_endpoint()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}